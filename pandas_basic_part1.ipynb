{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HF-BfBMnAyNU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd   #pd is not keyword.It is just a short form for pandas. We can use any name instead of pd. viz. pranjal,  mm,etc."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Some important concepts\n",
        "\n",
        " DATA FRAME- Whenever we are trying to import excel, json,csv data files into python using pandas, then python converts all the data present in those files into the table like structure and it is known as data frame."
      ],
      "metadata": {
        "id": "ErPRyle83U9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "e1ckzP9aHOVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some important pandas functions:\n",
        "# 1. df.head() - By default it will show the first five records present in the dataframe.\n",
        "\n",
        "# 2. df.tail() - By default it will show the last five records present in the dataframe.\n",
        "# If we want to display only n items or records then we can use accordingly df.head(n) or df.tail(n) where n represnets the no of records you want to display or show.\n",
        "\n",
        "# 3. df.columns - It will display all the column names present in the file which we have imported in the python.\n",
        "\n",
        "# 4.if we want to see the entire dataset of the particular column then we have to follow this:\n",
        "#      df.['Column name'],         where column name refers to the column whose entire data you have to fetch.\n",
        "\n",
        "# 5.If we want to see the entire data set of the more than one column, then we have to pass the names of the entire data set in the list like this:\n",
        "#     df.[['Column Name1','Column name 2',\"Col name 3\"]]\n",
        "# 6 .All type of operations(Slicing),etc which is poosible."
      ],
      "metadata": {
        "id": "oXXAt2TYn-7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONCEPT 1. UPLOADING THE CSV FILE PRESENT IN THE LOCAL SYSTEM."
      ],
      "metadata": {
        "id": "mH-KThWA85J2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "GrTmHqv95i3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "df2 = pd.read_csv(io.BytesIO(uploaded['Admission_Prediction.csv']))\n",
        "df2"
      ],
      "metadata": {
        "id": "T-40zK6u5vOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DISPLAYING THE FIRST 5 DATASETS PRESENT IN OUR DATAFRAME d2"
      ],
      "metadata": {
        "id": "C108U-qx9DKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head()"
      ],
      "metadata": {
        "id": "Oyvd_wws9KQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the First 3 dataset present in the dataframe df2"
      ],
      "metadata": {
        "id": "yj0oKSCn9Lvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head(3)"
      ],
      "metadata": {
        "id": "tuoglogp9f_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying all the columns present in the dataframe df2"
      ],
      "metadata": {
        "id": "a5rNnTP_9iGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.columns"
      ],
      "metadata": {
        "id": "bBLpFwHoexlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Displaying the entire data of the Particular Column."
      ],
      "metadata": {
        "id": "giDzfmB-ey_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#COLUMNS OPERATIONS"
      ],
      "metadata": {
        "id": "HAjN8l4jVDud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df2[['GRE Score']]) #  It will try to include the name of the column also\n",
        "print(df2['GRE Score'])   #  It will try yto diplay the same result without the name of the column."
      ],
      "metadata": {
        "id": "WB5LL9IFymoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(df2['GRE Score'])  #It is of the type Series."
      ],
      "metadata": {
        "id": "7Ra8evAr5DIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " type(df2[['GRE Score']]) # It is of the type Dataframe."
      ],
      "metadata": {
        "id": "Frqibz2K5Ho_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Displaying the entire data of the Some Columns."
      ],
      "metadata": {
        "id": "SqwEo_32ysBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2[['GRE Score','SOP','Chance of Admit']]"
      ],
      "metadata": {
        "id": "7Hfw-xUxzB0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Slicing Opertions of data frame in Columns data."
      ],
      "metadata": {
        "id": "eYaVQuBHUZDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2[['GRE Score','SOP','Chance of Admit']][0:8]  # It will display all the results of dataset upto 7."
      ],
      "metadata": {
        "id": "reP66lxPUeEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2[['GRE Score','SOP']][0:8]"
      ],
      "metadata": {
        "id": "ZKfRVYbkUtG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now retrieving the data present at the even location for GRE Score And SOP"
      ],
      "metadata": {
        "id": "cE5nQfWDVTj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2[['GRE Score','SOP']][0:: 2]"
      ],
      "metadata": {
        "id": "I_8zuTvDXFGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now retrieving the data present at the odd location for GRE Score And SOP"
      ],
      "metadata": {
        "id": "3QGziFNlRquA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2[['GRE Score','SOP']][0::]"
      ],
      "metadata": {
        "id": "SNllmOe8RuDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Imp Concept: In series we have indexes and python recognizes that indixes too.We can call the column via indexesas shown below. \n",
        "df2['GRE Score'][0]"
      ],
      "metadata": {
        "id": "v-0eZtX2Uive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Imp Concept: In dataframes we have indexes and but python  dont recognizes that indixes.We cannot  call the column via indexesas shown below.\n",
        "df2[['GRE Score']][0]"
      ],
      "metadata": {
        "id": "1IxM6LbFfvFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# But the slicing operations works efficiently in both of them as shown below:\n",
        "print(df2['GRE Score'][0:9])\n",
        "print(df2[['GRE Score']][0:9])"
      ],
      "metadata": {
        "id": "y2mZJWFhf-X0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Very important note: If we have file other than CSV(Comma seperated values), then we have to include sep= \"special character(@,&)\" \n",
        "# while calling the file in python as shown below:\n",
        "# df2 = pd.read_csv(io.BytesIO(uploaded['Admission_Prediction.csv'],sep=\"@\"))."
      ],
      "metadata": {
        "id": "_voZpON_sx8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CONCEPT 2: Reading multiple worksheets in Excel."
      ],
      "metadata": {
        "id": "Wj9oyHTCm3OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "ZSpFkqVAmpUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xls = pd.ExcelFile(\"Weekly_Project_Status.xlsx\")\n",
        "df1 = pd.read_excel(xls, 'P1')\n",
        "df2 = pd.read_excel(xls, 'P2')\n",
        "df3 = pd.read_excel(xls, 'P3')"
      ],
      "metadata": {
        "id": "e1amEnhZMiTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1"
      ],
      "metadata": {
        "id": "k6tn4DdDlp8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2"
      ],
      "metadata": {
        "id": "kZRA54XelqrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3"
      ],
      "metadata": {
        "id": "1dDkYC7ZlrP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONCEPT 3: Reading the CSV Files from Github."
      ],
      "metadata": {
        "id": "iytI-wdJn3_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df4=pd.read_csv(\"https://raw.githubusercontent.com/codeforamerica/ohana-api/master/data/sample-csv/addresses.csv\")"
      ],
      "metadata": {
        "id": "IYbNn__sp40f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df4"
      ],
      "metadata": {
        "id": "oi8hDcMdsjY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(df4)"
      ],
      "metadata": {
        "id": "gsvs7SUMA6ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Following are the steps:\n",
        "# 1. Just go to the git hub and enter the file which u want to upload in python.\n",
        "# 2. After entering, there will be raw option available in the right sids of the github.\n",
        "# 3. After entering the raw option, comma separated file will be generated and just copy the url of that file and do the following thing:\n",
        "#    df4=pd.read_csv(\"https://raw.githubusercontent.com/codeforamerica/ohana-api/master/data/sample-csv/addresses.csv\")\n",
        "# 4. File will be uploaded in the python and we can call the required dataframe."
      ],
      "metadata": {
        "id": "0ZGvoj9VskMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONCEPT 4 : Reading the HTML Files.\n",
        "# Python always try to import the html files in the form of list.Inside list, it contains multiple tables and each table are called the dataframes.\n",
        "# Whatever be the operations which we have performed above is on the dataframes. \n",
        "# and in order to access the dataframes inside the list, we have to perfrom the index opearions as we have done in the list concepts."
      ],
      "metadata": {
        "id": "Cp00ZPeawMFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5= pd.read_html(\"https://www.basketball-reference.com/leagues/NBA_2015_totals.html\")\n",
        "# The main issue with the pd.read_html is that the python will read the data of the html file which is present inside the header tag(only tabular dataset).If the data is \n",
        "# not present inside the header, then no data will be imported in the python.Thats why some of the data are still misisng while importing the above file.\n",
        "# if we want to import the images, etc of the html files, then we have to follow the different approach.\n",
        "df5\n",
        "type(df5)"
      ],
      "metadata": {
        "id": "Hsy8I4DR_ZCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to find out the no of tables availabe in our dataframe, then we have to use the len function.\n",
        "len(df5)"
      ],
      "metadata": {
        "id": "lm9zSh0Q16bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now only one table is available inside the dataframe df5.  "
      ],
      "metadata": {
        "id": "PpB2ekuJ-kGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#In order to extract the html data,we have to perform the operations of list.\n",
        "#In df5 only one table is present hence, we can extract using 0 as the index nyumber of df5."
      ],
      "metadata": {
        "id": "1HDUHhzi_B1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we are able to extract the data in the same format as it is present inside the html file."
      ],
      "metadata": {
        "id": "D3I1j3Ze_iy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we will be performing all the pandas operations in df5[0] only.\n",
        "df5[0].head()\n",
        "df5[0].tail() \n",
        "df5[0][['Player','TRB']][0:10]\n",
        "df5[0].columns"
      ],
      "metadata": {
        "id": "ynTKyypo_sb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OPEARIONS IN DATA FRAME"
      ],
      "metadata": {
        "id": "B6Q8BLnkEhsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.Knowing the type of each columns present in the dataset."
      ],
      "metadata": {
        "id": "BipdEGvEKtT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5[0].dtypes   # dtypes- is used to find out the datatyps of each column present in the dataset."
      ],
      "metadata": {
        "id": "hex3hDx7Kzu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Syntax for installing the pandas profiling in pandas."
      ],
      "metadata": {
        "id": "C7ZAtt4YLvB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas-profiling # Syntax for installing the pandas profiling."
      ],
      "metadata": {
        "id": "AbV2v7eyLqm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  import numpy as np\n",
        "  x= np.array([[3.2,7.8,9.2],\n",
        "               [4.5,9.1,1.2]], dtype='int64')\n",
        "  print(x.itemsize)\n",
        "  print(x)"
      ],
      "metadata": {
        "id": "qqKRQfB2Lt7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MERGING 2 DATA FRAMES IN PANDAS**"
      ],
      "metadata": {
        "id": "3Y2-jJgVRvf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df2 = pd.DataFrame({'Name':['Toyato','audi','Honda','Infinite'],\n",
        "                   'Manufacturing_date':[1990,1998,1980,2000],\n",
        "                   })\n",
        "df1 = pd.DataFrame({'Name':['Toyato','Honda','Infinite','Jeep'],\n",
        "                   'Starting_price':['37k','30k','50k','45k'],\n",
        "                    })\n",
        "print(df1)\n",
        "\n",
        "print(df2)"
      ],
      "metadata": {
        "id": "KEIgRkORRx9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Case 1 : Applying right join on the above 2 tables"
      ],
      "metadata": {
        "id": "JeG6jtSLUw5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.merge(df1,df2,how ='right')"
      ],
      "metadata": {
        "id": "JeFEwnaZStuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Case 2 : Applying left join on the above 2 tables"
      ],
      "metadata": {
        "id": "baFC4xDJT8pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.merge(df1,df2,how ='left')"
      ],
      "metadata": {
        "id": "dgeesNzqVCFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Case 3 : Applying inner join on the above 2 tables"
      ],
      "metadata": {
        "id": "YK7JpgBLVXsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.merge(df1,df2,how ='inner')"
      ],
      "metadata": {
        "id": "O-1WdICgVesM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Case 4 : Applying outer join on the above 2 tables"
      ],
      "metadata": {
        "id": "kb1-4SjYVgah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.merge(df1,df2,how ='outer')"
      ],
      "metadata": {
        "id": "r4ngWKaBVnng"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}